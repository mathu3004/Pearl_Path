{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathu3004/Pearl_Path/blob/main/Pre_processing_attraction_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W653wDjQvrTf",
        "outputId": "561cb2d2-1d46-4243-f54a-8305d3c198ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Null Counts:\n",
            "File 1:\n",
            "address                    0\n",
            "addressObj/city            0\n",
            "addressObj/country         0\n",
            "addressObj/postalcode     79\n",
            "addressObj/state         300\n",
            "                        ... \n",
            "subtype/45               299\n",
            "travelerChoiceAward      300\n",
            "type                       0\n",
            "webUrl                     0\n",
            "website                   22\n",
            "Length: 219, dtype: int64\n",
            "File 2:\n",
            "address                    0\n",
            "addressObj/city            1\n",
            "addressObj/country         0\n",
            "addressObj/postalcode    105\n",
            "addressObj/state         300\n",
            "                        ... \n",
            "subtype/25               299\n",
            "travelerChoiceAward      300\n",
            "type                       0\n",
            "webUrl                     0\n",
            "website                   83\n",
            "Length: 201, dtype: int64\n",
            "File 3:\n",
            "address                    0\n",
            "addressObj/city            1\n",
            "addressObj/country         0\n",
            "addressObj/postalcode    111\n",
            "addressObj/state         296\n",
            "                        ... \n",
            "subtype/22               299\n",
            "travelerChoiceAward      300\n",
            "type                       0\n",
            "webUrl                     0\n",
            "website                  105\n",
            "Length: 200, dtype: int64\n",
            "File 4:\n",
            "address                    0\n",
            "addressObj/city            0\n",
            "addressObj/country         0\n",
            "addressObj/postalcode     33\n",
            "addressObj/state         171\n",
            "                        ... \n",
            "subtype/45               170\n",
            "travelerChoiceAward      171\n",
            "type                       0\n",
            "webUrl                     0\n",
            "website                   67\n",
            "Length: 289, dtype: int64\n",
            "\n",
            "Original Duplicate Counts:\n",
            "File 1: 0\n",
            "File 2: 0\n",
            "File 3: 0\n",
            "File 4: 0\n",
            "\n",
            "Original Row Counts:\n",
            "File 1: 300\n",
            "File 2: 300\n",
            "File 3: 300\n",
            "File 4: 171\n",
            "\n",
            "Processed Null Counts:\n",
            "Name                   0\n",
            "Address                0\n",
            "City                   0\n",
            "Province               0\n",
            "Category               0\n",
            "Description            0\n",
            "Website                0\n",
            "Phone                  0\n",
            "Email                  0\n",
            "Booking URL            0\n",
            "Latitude               0\n",
            "Longitude              0\n",
            "Ranking_Position       0\n",
            "Rating                55\n",
            "Lowest Price           0\n",
            "Review URL             0\n",
            "Review Count           0\n",
            "Sub Categories         0\n",
            "Sub Types              0\n",
            "Primary Category 1     0\n",
            "Title 1                0\n",
            "Price 1                0\n",
            "URL 1                  0\n",
            "Primary Category 2     0\n",
            "Title 2                0\n",
            "Price 2                0\n",
            "URL 2                  0\n",
            "Primary Category 3     0\n",
            "Title 3                0\n",
            "Price 3                0\n",
            "URL 3                  0\n",
            "Primary Category 4     0\n",
            "Title 4                0\n",
            "Price 4                0\n",
            "URL 4                  0\n",
            "dtype: int64\n",
            "\n",
            "Processed Duplicate Counts:\n",
            "42\n",
            "\n",
            "Processed Row Count:\n",
            "914\n",
            "\n",
            "New Row Count (after removing duplicates and nulls):\n",
            "817\n",
            "Processing complete. Merged cleaned data saved to 'merged_processed_data_Attractions.xlsx'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to clean and process a single DataFrame\n",
        "def clean_and_process_df(df):\n",
        "    # Standardize column names\n",
        "    df.columns = [col.strip().lower().replace(' ', '').replace('/', '') for col in df.columns]\n",
        "\n",
        "    # Ensure all required columns exist, filling missing ones with NaN\n",
        "    required_columns = [\n",
        "        'name', 'address', 'addressobjcity', 'ancestorlocations1name', 'category', 'description',\n",
        "        'website', 'phone', 'email', 'bookingurl', 'latitude', 'longitude', 'rankingposition',\n",
        "        'rating', 'offergrouplowestprice', 'weburl', 'numberofreviews',\n",
        "        'subcategories0', 'subcategories1', 'subcategories2', 'subcategories3', 'subcategories4',\n",
        "        'subcategories5', 'subcategories6', 'subcategories7', 'subcategories8', 'subcategories9',\n",
        "        'subtype0', 'subtype1', 'subtype2', 'subtype3', 'subtype4', 'subtype5', 'subtype6', 'subtype7',\n",
        "        'subtype8', 'subtype9', 'subtype10', 'subtype11', 'subtype12', 'subtype13', 'subtype14',\n",
        "        'subtype15', 'subtype16', 'subtype17', 'subtype18', 'subtype19', 'subtype20', 'subtype21',\n",
        "        'subtype22', 'subtype23', 'subtype24', 'subtype25', 'subtype26', 'subtype27', 'subtype28',\n",
        "        'subtype29', 'subtype30', 'subtype31', 'subtype32', 'subtype33', 'subtype34', 'subtype35',\n",
        "        'subtype36', 'subtype37', 'subtype38', 'subtype39', 'subtype40', 'subtype41', 'subtype42',\n",
        "        'subtype43', 'subtype44', 'subtype45',\n",
        "        'offergroupofferlist0primarycategory', 'offergroupofferlist0title', 'offergroupofferlist0price', 'offergroupofferlist0url',\n",
        "        'offergroupofferlist1primarycategory', 'offergroupofferlist1title', 'offergroupofferlist1price', 'offergroupofferlist1url',\n",
        "        'offergroupofferlist2primarycategory', 'offergroupofferlist2title', 'offergroupofferlist2price', 'offergroupofferlist2url',\n",
        "        'offergroupofferlist3primarycategory', 'offergroupofferlist3title', 'offergroupofferlist3price', 'offergroupofferlist3url'\n",
        "    ]\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "\n",
        "    # Rename columns as specified\n",
        "    df.rename(columns={\n",
        "        'name': 'Name',\n",
        "        'address': 'Address',\n",
        "        'addressobjcity': 'City',\n",
        "        'ancestorlocations1name': 'Province',\n",
        "        'category': 'Category',\n",
        "        'description': 'Description',\n",
        "        'website': 'Website',\n",
        "        'phone': 'Phone',\n",
        "        'email': 'Email',\n",
        "        'bookingurl': 'Booking URL',\n",
        "        'latitude': 'Latitude',\n",
        "        'longitude': 'Longitude',\n",
        "        'rankingposition': 'Ranking_Position',\n",
        "        'rating': 'Rating',\n",
        "        'offergrouplowestprice': 'Lowest Price',\n",
        "        'weburl': 'Review URL',\n",
        "        'numberofreviews': 'Review Count'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Filter rows based on the 'City' column\n",
        "    valid_cities = ['Kandy', 'Nuwara Eliya', 'Ella', 'Colombo']\n",
        "    df = df[df['City'].isin(valid_cities)]\n",
        "\n",
        "    # Process the 'Description' column\n",
        "    df['Description'] = df['Description'].fillna('no description')\n",
        "\n",
        "    # Process the 'Website' column\n",
        "    df['Website'] = df['Website'].fillna('no website')\n",
        "\n",
        "    # Process the 'Phone' column\n",
        "    df['Phone'] = df['Phone'].apply(lambda x: x if (isinstance(x, str) and ('+' in x or x.isdigit())) else 'no phone')\n",
        "\n",
        "    # Process the 'Email' column\n",
        "    df['Email'] = df['Email'].fillna('no email')\n",
        "\n",
        "    # Process the 'Booking URL' column\n",
        "    df['Booking URL'] = df['Booking URL'].fillna('no booking url')\n",
        "\n",
        "    # Process the 'Latitude' and 'Longitude' columns\n",
        "    df['Latitude'] = df['Latitude'].fillna('no latitude')\n",
        "    df['Longitude'] = df['Longitude'].fillna('no longitude')\n",
        "\n",
        "    # Process the 'Lowest Price' column\n",
        "    df['Lowest Price'] = df['Lowest Price'].fillna('No price mentioned')\n",
        "\n",
        "    # Process the 'Review Count' column\n",
        "    df['Review Count'] = df['Review Count'].fillna(0).astype(int)\n",
        "\n",
        "    # Combine subcategories into one column\n",
        "    subcategories_cols = [f'subcategories{i}' for i in range(10)]\n",
        "    df['Sub Categories'] = df[subcategories_cols].apply(lambda row: ','.join(row.dropna()) if not row.isnull().all() else 'no sub category', axis=1)\n",
        "\n",
        "    # Combine subtypes into one column\n",
        "    subtypes_cols = [f'subtype{i}' for i in range(46)]\n",
        "    df['Sub Types'] = df[subtypes_cols].apply(lambda row: ','.join(row.dropna()) if not row.isnull().all() else 'no sub type', axis=1)\n",
        "\n",
        "    # Process the 'Ranking_Position' column\n",
        "    df['Ranking_Position'] = df['Ranking_Position'].fillna('no ranking')\n",
        "\n",
        "    # Process the repeated offerGroup/offerList columns\n",
        "    for i in range(4):\n",
        "        primary_category_col = f'offergroupofferlist{i}primarycategory'\n",
        "        title_col = f'offergroupofferlist{i}title'\n",
        "        price_col = f'offergroupofferlist{i}price'\n",
        "        url_col = f'offergroupofferlist{i}url'\n",
        "\n",
        "        df[f'Primary Category {i+1}'] = df[primary_category_col].fillna('No primary category')\n",
        "        df[f'Title {i+1}'] = df[title_col].fillna('No title')\n",
        "        df[f'Price {i+1}'] = df[price_col].fillna('No price')\n",
        "        df[f'URL {i+1}'] = df[url_col].fillna('No url')\n",
        "\n",
        "    # Drop the original subcategories and subtypes columns\n",
        "    df.drop(columns=subcategories_cols + subtypes_cols, inplace=True)\n",
        "\n",
        "    # Select and reorder columns as per the requirements\n",
        "    output_df = df[[\n",
        "        'Name', 'Address', 'City', 'Province', 'Category', 'Description', 'Website', 'Phone', 'Email',\n",
        "        'Booking URL', 'Latitude', 'Longitude', 'Ranking_Position', 'Rating', 'Lowest Price', 'Review URL',\n",
        "        'Review Count', 'Sub Categories', 'Sub Types', 'Primary Category 1', 'Title 1', 'Price 1', 'URL 1',\n",
        "        'Primary Category 2', 'Title 2', 'Price 2', 'URL 2', 'Primary Category 3', 'Title 3', 'Price 3', 'URL 3',\n",
        "        'Primary Category 4', 'Title 4', 'Price 4', 'URL 4'\n",
        "    ]]\n",
        "\n",
        "    return output_df\n",
        "\n",
        "# Load the datasets\n",
        "file_paths = [\n",
        "    '/content/AttractionsColombo.xlsx',\n",
        "    '/content/AttractionsElla.xlsx',\n",
        "    '/content/AttractionsKandy.xlsx',\n",
        "    '/content/AttractionsNuwaraEliya.xlsx',\n",
        "]\n",
        "\n",
        "# Check for null values and duplicates before processing\n",
        "original_null_counts = []\n",
        "original_duplicate_counts = []\n",
        "original_row_counts = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    if file_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(file_path)\n",
        "        original_null_counts.append(df.isnull().sum())\n",
        "        original_duplicate_counts.append(df.duplicated().sum())\n",
        "        original_row_counts.append(len(df))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format for file: {file_path}\")\n",
        "\n",
        "# Clean and process each dataset\n",
        "cleaned_dfs = []\n",
        "for file_path in file_paths:\n",
        "    if file_path.endswith('.xlsx'):\n",
        "        cleaned_dfs.append(clean_and_process_df(pd.read_excel(file_path)))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format for file: {file_path}\")\n",
        "\n",
        "# Concatenate all cleaned DataFrames into one\n",
        "merged_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
        "\n",
        "# Check for null values and duplicates after processing\n",
        "processed_null_counts = merged_df.isnull().sum()\n",
        "processed_duplicate_counts = merged_df.duplicated().sum()\n",
        "processed_row_count = len(merged_df)\n",
        "\n",
        "# Remove duplicates and rows with null values\n",
        "merged_df.drop_duplicates(inplace=True)\n",
        "merged_df.dropna(inplace=True)\n",
        "\n",
        "# Get the new row count\n",
        "new_row_count = len(merged_df)\n",
        "\n",
        "# Save the merged DataFrame to a new Excel file\n",
        "output_file = 'merged_processed_data_Attractions.xlsx'\n",
        "merged_df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Null Counts:\")\n",
        "for i, null_count in enumerate(original_null_counts):\n",
        "    print(f\"File {i+1}:\")\n",
        "    print(null_count)\n",
        "\n",
        "print(\"\\nOriginal Duplicate Counts:\")\n",
        "for i, dup_count in enumerate(original_duplicate_counts):\n",
        "    print(f\"File {i+1}: {dup_count}\")\n",
        "\n",
        "print(\"\\nOriginal Row Counts:\")\n",
        "for i, row_count in enumerate(original_row_counts):\n",
        "    print(f\"File {i+1}: {row_count}\")\n",
        "\n",
        "print(\"\\nProcessed Null Counts:\")\n",
        "print(processed_null_counts)\n",
        "\n",
        "print(\"\\nProcessed Duplicate Counts:\")\n",
        "print(processed_duplicate_counts)\n",
        "\n",
        "print(\"\\nProcessed Row Count:\")\n",
        "print(processed_row_count)\n",
        "\n",
        "print(\"\\nNew Row Count (after removing duplicates and nulls):\")\n",
        "print(new_row_count)\n",
        "\n",
        "print(f\"Processing complete. Merged cleaned data saved to '{output_file}'\")\n"
      ]
    }
  ]
}