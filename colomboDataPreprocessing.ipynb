{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/colomboHotels.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows to understand the structure of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Renaming Columns for Consistency: Replace '/' with '_'\n",
        "# This step standardizes column names by replacing '/' with '_' to avoid issues in processing\n",
        "df.columns = [col.strip().lower().replace(' ', '_').replace('/', '_') for col in df.columns]\n",
        "\n",
        "# Combine amenities columns into a single column\n",
        "# Identify columns starting with 'amenities_' (after renaming)\n",
        "amenities_columns = [col for col in df.columns if col.startswith('amenities_')]\n",
        "\n",
        "# Create a new column 'amenities' by concatenating all the 'amenities_' columns, separated by commas\n",
        "# Non-NaN values in the 'amenities_' columns are joined for each row\n",
        "df['amenities'] = df[amenities_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original amenities columns\n",
        "# Remove the now redundant 'amenities_' columns to keep the dataset clean\n",
        "df = df.drop(columns=amenities_columns)\n",
        "\n",
        "# Append 'addressobj_postalcode' to 'address', separated by a comma, if the column exists\n",
        "# This step checks if 'addressobj_postalcode' is in the dataframe\n",
        "if 'addressobj_postalcode' in df.columns:\n",
        "    # Combine 'address' and 'addressobj_postalcode', separating them with a comma if both are non-empty\n",
        "    df['address'] = df.apply(\n",
        "        lambda row: f\"{row['address']}, {row['addressobj_postalcode']}\" if pd.notna(row['addressobj_postalcode']) else row['address'],\n",
        "        axis=1\n",
        "    )\n",
        "    # Drop the 'addressobj_postalcode' column after appending its data\n",
        "    df = df.drop(columns=['addressobj_postalcode'])\n",
        "\n",
        "# Combine 'addressobj_street2' into 'addressobj_street1' and rename it to 'addressobj_street'\n",
        "# This step checks if 'addressobj_street2' is in the dataframe\n",
        "if 'addressobj_street2' in df.columns and 'addressobj_street1' in df.columns:\n",
        "    # Combine 'addressobj_street1' and 'addressobj_street2', separated by a comma if both are non-empty\n",
        "    df['addressobj_street1'] = df.apply(\n",
        "        lambda row: f\"{row['addressobj_street1']}, {row['addressobj_street2']}\" if pd.notna(row['addressobj_street2']) else row['addressobj_street1'],\n",
        "        axis=1\n",
        "    )\n",
        "    # Rename 'addressobj_street1' to 'addressobj_street'\n",
        "    df.rename(columns={'addressobj_street1': 'addressobj_street'}, inplace=True)\n",
        "    # Drop the 'addressobj_street2' column after appending its data\n",
        "    df = df.drop(columns=['addressobj_street2'])\n",
        "\n",
        "# Drop unnecessary columns related to ancestor locations\n",
        "# These columns are no longer needed in the dataset\n",
        "columns_to_drop = [\n",
        "    'ancestorlocations_0_id', 'ancestorlocations_0_name', 'ancestorlocations_0_subcategory',\n",
        "    'ancestorlocations_1_id', 'ancestorlocations_1_name', 'ancestorlocations_1_subcategory',\n",
        "    'ancestorlocations_2_id', 'ancestorlocations_2_name', 'ancestorlocations_2_subcategory',\n",
        "    'ancestorlocations_3_id', 'ancestorlocations_3_name', 'ancestorlocations_3_subcategory',\n",
        "    'checkindate', 'checkoutdate', 'input', 'isnearbyresult', 'photocount', 'roomtips_0_user',\n",
        "    'roomtips_1_user', 'roomtips_2_user', 'roomtips_3_user', 'roomtips_4_user',\n",
        "    'whatsappredirecturl'\n",
        "]\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "\n",
        "\n",
        "# Update certain columns if they are empty\n",
        "# Combine 'weburl' and 'website' values if present and append to a default message\n",
        "if 'description' in df.columns:\n",
        "    df['description'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['description']) else row['description']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'email' in df.columns:\n",
        "    df['email'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['email']) else row['email']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'phone' in df.columns:\n",
        "    df['phone'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['phone']) else row['phone']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'pricelevel' in df.columns:\n",
        "    df['pricelevel'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['pricelevel']) else row['pricelevel']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'pricerange' in df.columns:\n",
        "    df['pricerange'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['pricerange']) else row['pricerange']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'website' in df.columns:\n",
        "    df['website'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['website']) else row['website']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Combine review and text columns for each review tag into a single column\n",
        "# Iterate through pairs of 'reviewtags_*_reviews' and 'reviewtags_*_text'\n",
        "review_columns = [col for col in df.columns if col.startswith('reviewtags_') and ('_reviews' in col or '_text' in col)]\n",
        "review_pairs = {}\n",
        "\n",
        "# Group review and text columns into pairs (e.g., 'reviewtags_0_reviews', 'reviewtags_0_text')\n",
        "for col in review_columns:\n",
        "    key = col.split('_')[1]  # Extract the common index for pairing\n",
        "    review_pairs.setdefault(key, []).append(col)\n",
        "\n",
        "# Create a new column 'reviews_and_text' to store combined reviews and text\n",
        "df['reviews_and_text'] = df.apply(\n",
        "    lambda row: ', '.join(\n",
        "        f\"{row[pair[0]]}: {row[pair[1]]}\" for pair in review_pairs.values()\n",
        "        if len(pair) == 2 and pd.notna(row[pair[0]]) and pd.notna(row[pair[1]])\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Drop original review and text columns to clean the dataset\n",
        "df = df.drop(columns=review_columns)\n",
        "\n",
        "# Combine roomtips_* columns into a single column if they exist\n",
        "# Identify columns starting with 'roomtips_'\n",
        "roomtips_columns = [col for col in df.columns if col.startswith('roomtips_')]\n",
        "\n",
        "if roomtips_columns:  # Check if any 'roomtips_*' columns exist\n",
        "    # Group roomtips columns into sets of six based on the shared index (e.g., 'roomtips_0_', 'roomtips_1_')\n",
        "    roomtips_groups = {}\n",
        "    for col in roomtips_columns:\n",
        "        key = col.split('_')[1]  # Extract the common index for grouping (e.g., '0', '1')\n",
        "        roomtips_groups.setdefault(key, []).append(col)\n",
        "\n",
        "    # Create a new column 'roomtips' to store combined room tips for each row\n",
        "    df['roomtips'] = df.apply(\n",
        "        lambda row: ', '.join(\n",
        "            ', '.join(\n",
        "                str(row[col]) for col in sorted(group) if pd.notna(row[col])\n",
        "            ) for group in roomtips_groups.values()\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Drop the original roomtips_* columns to clean the dataset\n",
        "    df = df.drop(columns=roomtips_columns)\n",
        "else:\n",
        "    # If no 'roomtips_*' columns exist, create an empty 'roomtips' column\n",
        "    df['roomtips'] = ''\n",
        "\n",
        "# Replace empty values in specified columns with 'not provided'\n",
        "# For the specified columns, replace missing values (NaN) with 'not provided' or '0' for scores\n",
        "columns_to_replace = [\n",
        "    'categoryreviewscores_0_categoryname', 'categoryreviewscores_0_score',\n",
        "    'categoryreviewscores_1_categoryname', 'categoryreviewscores_1_score',\n",
        "    'categoryreviewscores_2_categoryname', 'categoryreviewscores_2_score',\n",
        "    'categoryreviewscores_3_categoryname', 'categoryreviewscores_3_score',\n",
        "    'categoryreviewscores_4_categoryname', 'categoryreviewscores_4_score',\n",
        "    'categoryreviewscores_5_categoryname', 'categoryreviewscores_5_score',\n",
        "    'hotelclassattribution', 'latitude', 'longitude', 'numberofrooms', 'travelerchoiceaward'\n",
        "]\n",
        "\n",
        "# Replace missing values: text columns with 'not provided', numeric score columns with 0\n",
        "for col in columns_to_replace:\n",
        "    if col in df.columns:\n",
        "        if col.endswith('score'):  # If the column is a score, replace NaN with 0\n",
        "            df[col] = df[col].fillna(0)\n",
        "        else:  # Otherwise, replace NaN with 'not provided'\n",
        "            df[col] = df[col].fillna('not provided')\n",
        "\n",
        "# Replace empty values in 'amenities' and 'reviews_and_text' columns with 'not provided'\n",
        "# Check if the columns exist and replace missing values with 'not provided'\n",
        "if 'amenities' in df.columns:\n",
        "    df['amenities'] = df['amenities'].replace('', 'not provided')\n",
        "\n",
        "if 'reviews_and_text' in df.columns:\n",
        "    df['reviews_and_text'] = df['reviews_and_text'].replace('', 'not provided')\n",
        "if 'roomtips' in df.columns:\n",
        "    df['roomtips'] = df['roomtips'].replace(',,,,', 'not provided')\n",
        "\n",
        "# Save the modified data to a new Excel file\n",
        "# The preprocessed file contains standardized column names, a combined 'amenities' column,\n",
        "# an updated 'address' column, and an updated 'addressobj_street' column\n",
        "output_file = '/content/preprocessed_hotel_data.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Preprocessing complete. Cleaned data saved to '{output_file}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlwFHe4KxHJe",
        "outputId": "6148372f-98e1-41a3-e090-52fb9ee7949e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "                                             address addressObj/city  \\\n",
            "0  14 Borella Cross Road off Ward Place, Colombo ...         Colombo   \n",
            "1  538 Galle Road Colombo 03, Colombo 00300 Sri L...         Colombo   \n",
            "2        No 7/1 Elias Place, Colombo 01000 Sri Lanka         Colombo   \n",
            "3  29 Milagiriya Avenue Marine Drive, Colombo 004...         Colombo   \n",
            "4        28, Borella Cross Road, Colombo 8 Sri Lanka         Colombo   \n",
            "\n",
            "  addressObj/country addressObj/postalcode      addressObj/street1  \\\n",
            "0          Sri Lanka                 00800   14 Borella Cross Road   \n",
            "1          Sri Lanka                 00300          538 Galle Road   \n",
            "2          Sri Lanka                 01000      No 7/1 Elias Place   \n",
            "3          Sri Lanka                 00400    29 Milagiriya Avenue   \n",
            "4          Sri Lanka                     8  28, Borella Cross Road   \n",
            "\n",
            "  addressObj/street2 amenities/0    amenities/1    amenities/2  \\\n",
            "0     off Ward Place    Internet  Free Internet     Restaurant   \n",
            "1         Colombo 03    Internet   Room service  Free Internet   \n",
            "2                NaN    Internet  Free Internet   Free parking   \n",
            "3       Marine Drive        Wifi    Public Wifi      Free Wifi   \n",
            "4                NaN    Internet  Free Internet   Free parking   \n",
            "\n",
            "              amenities/3  ... roomTips/4/reviewId  \\\n",
            "0  Airport transportation  ...         559981172.0   \n",
            "1       Wheelchair access  ...         654352208.0   \n",
            "2                    Wifi  ...                 NaN   \n",
            "3            Dry Cleaning  ...         559207901.0   \n",
            "4        Air conditioning  ...         167307848.0   \n",
            "\n",
            "                                     roomTips/4/text roomTips/4/type  \\\n",
            "0  There are rooms located to the street and room...        room_tip   \n",
            "1                                      Go elsewhere.        room_tip   \n",
            "2                                                NaN             NaN   \n",
            "3                        Choose room with ocean view        room_tip   \n",
            "4  Eat out, take a bed sheet and mosquito repelle...        room_tip   \n",
            "\n",
            "  roomTips/4/user    subcategories/0 travelerChoiceAward   type  \\\n",
            "0             NaN  Bed and Breakfast                 NaN  HOTEL   \n",
            "1             NaN  Bed and Breakfast       REGULAR_AWARD  HOTEL   \n",
            "2             NaN  Specialty Lodging                 NaN  HOTEL   \n",
            "3             NaN              Hotel                 NaN  HOTEL   \n",
            "4             NaN  Bed and Breakfast                 NaN  HOTEL   \n",
            "\n",
            "                                              webUrl  \\\n",
            "0  https://www.tripadvisor.com/Hotel_Review-g2939...   \n",
            "1  https://www.tripadvisor.com/Hotel_Review-g2939...   \n",
            "2  https://www.tripadvisor.com/Hotel_Review-g2939...   \n",
            "3  https://www.tripadvisor.com/Hotel_Review-g2939...   \n",
            "4  https://www.tripadvisor.com/Hotel_Review-g2939...   \n",
            "\n",
            "                                             website whatsAppRedirectUrl  \n",
            "0                         http://RockwellColombo.com                 NaN  \n",
            "1                             http://www.ivylane.lk/                 NaN  \n",
            "2  https://www.facebook.com/Steps-Backpackers-Hos...                 NaN  \n",
            "3                          http://hoteleurolanka.com                 NaN  \n",
            "4                    http://www.ceyvista.com/skyways                 NaN  \n",
            "\n",
            "[5 rows x 347 columns]\n",
            "Preprocessing complete. Cleaned data saved to '/content/preprocessed_hotel_data.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/vacationRentalsColombo.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows to understand the structure of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Renaming Columns for Consistency: Replace '/' with '_'\n",
        "# This step standardizes column names by replacing '/' with '_' to avoid issues in processing\n",
        "df.columns = [col.strip().lower().replace(' ', '_').replace('/', '_') for col in df.columns]\n",
        "\n",
        "# Combine amenities columns into a single column\n",
        "# Identify columns starting with 'amenities_' (after renaming)\n",
        "amenities_columns = [col for col in df.columns if col.startswith('amenities_')]\n",
        "\n",
        "# Create a new column 'amenities' by concatenating all the 'amenities_' columns, separated by commas\n",
        "# Non-NaN values in the 'amenities_' columns are joined for each row\n",
        "df['amenities'] = df[amenities_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original amenities columns\n",
        "# Remove the now redundant 'amenities_' columns to keep the dataset clean\n",
        "df = df.drop(columns=amenities_columns)\n",
        "\n",
        "# Drop unnecessary columns related to ancestor locations\n",
        "# These columns are no longer needed in the dataset\n",
        "columns_to_drop = [\n",
        "   'rentaldescriptions_0_machinetranslated', 'input', 'photocount'\n",
        "]\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "\n",
        "# Replace empty values in specified columns with 'not provided' or 0\n",
        "columns_to_replace = [\n",
        "    'basedailyrate_amount', 'rating', 'rentaldescriptions_0_type'\n",
        "]\n",
        "\n",
        "# Replace missing values: 'rating' with 0, others with 'not provided'\n",
        "for col in columns_to_replace:\n",
        "    if col in df.columns:\n",
        "        if col == 'rating':  # Check the column name instead of using `equals`\n",
        "            df[col] = df[col].fillna(0)\n",
        "        else:\n",
        "            df[col] = df[col].fillna('not provided')\n",
        "\n",
        "\n",
        "# Replace empty 'rentalDescriptions_0_text' with a default message if the column exists and the value is empty\n",
        "if 'rentaldescriptions_0_text' in df.columns:\n",
        "    df['rentaldescriptions_0_text'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following link for more details: {', '.join(filter(pd.notna, [row.get('weburl', '')]))}\"\n",
        "            if pd.isna(row['rentaldescriptions_0_text']) or row['rentaldescriptions_0_text'] == ''\n",
        "            else row['rentaldescriptions_0_text']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# Save the modified data to a new Excel file\n",
        "# The preprocessed file contains standardized column names, a combined 'amenities' column,\n",
        "# an updated 'address' column, and an updated 'addressobj_street' column\n",
        "output_file = '/content/preprocessed_vacation_rental.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Preprocessing complete. Cleaned data saved to '{output_file}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlAtxpa3YPZ-",
        "outputId": "0b86a742-9bf3-4631-aea9-b21142b678b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "         amenities/0        amenities/1            amenities/2  \\\n",
            "0   Kid friendly: NO   Elder access: NO  Wheelchair access: NO   \n",
            "1  Kid friendly: YES   Elder access: NO  Wheelchair access: NO   \n",
            "2   Kid friendly: NO   Elder access: NO  Wheelchair access: NO   \n",
            "3  Kid friendly: YES   Elder access: NO  Wheelchair access: NO   \n",
            "4   Kid friendly: NO  Elder access: YES  Wheelchair access: NO   \n",
            "\n",
            "         amenities/3          amenities/4     amenities/5  amenities/6  \\\n",
            "0   Pet friendly: NO  Smoking allowed: NO  2 full bath(s)   Fits 8 pax   \n",
            "1  Pet friendly: YES  Smoking allowed: NO  2 full bath(s)   Fits 4 pax   \n",
            "2   Pet friendly: NO  Smoking allowed: NO  1 full bath(s)   Fits 2 pax   \n",
            "3   Pet friendly: NO  Smoking allowed: NO  5 full bath(s)  Fits 14 pax   \n",
            "4   Pet friendly: NO  Smoking allowed: NO  1 full bath(s)   Fits 2 pax   \n",
            "\n",
            "    amenities/7    amenities/8  baseDailyRate/amount  ...  \\\n",
            "0  2 bedroom(s)  2 bathroom(s)                 145.0  ...   \n",
            "1  2 bedroom(s)  2 bathroom(s)                  47.0  ...   \n",
            "2  1 bedroom(s)  1 bathroom(s)                  17.0  ...   \n",
            "3  7 bedroom(s)  5 bathroom(s)                 228.0  ...   \n",
            "4  1 bedroom(s)  1 bathroom(s)                  17.0  ...   \n",
            "\n",
            "                                                name numberOfReviews  \\\n",
            "0                      Hotel, beach and Water Sports               0   \n",
            "1  Serene Tropical Paradise: Your Dream Vacation ...               0   \n",
            "2                               Tahala  Transit Home               0   \n",
            "3                             Temco Holiday Bungalow               0   \n",
            "4                               Tahala  Transit Home               0   \n",
            "\n",
            "  numberOfRooms  photoCount rating  rentalDescriptions/0/machineTranslated  \\\n",
            "0             2          24    NaN                                   False   \n",
            "1             2           8    NaN                                   False   \n",
            "2             1           4    NaN                                   False   \n",
            "3             7          30    NaN                                   False   \n",
            "4             1           4    NaN                                   False   \n",
            "\n",
            "                           rentalDescriptions/0/text  \\\n",
            "0  Trincomalee is home to this hotel. Kandaswamy ...   \n",
            "1  Located in Nanaddan, this vacation home is by ...   \n",
            "2  Anuradhapura is home to this bed & breakfast. ...   \n",
            "3  Anuradhapura is home to this villa. Isurumuniy...   \n",
            "4  Anuradhapura is home to this apartment. The ar...   \n",
            "\n",
            "  rentalDescriptions/0/type             type  \\\n",
            "0   DESTINATION_DESCRIPTION  VACATION_RENTAL   \n",
            "1   DESTINATION_DESCRIPTION  VACATION_RENTAL   \n",
            "2   DESTINATION_DESCRIPTION  VACATION_RENTAL   \n",
            "3   DESTINATION_DESCRIPTION  VACATION_RENTAL   \n",
            "4   DESTINATION_DESCRIPTION  VACATION_RENTAL   \n",
            "\n",
            "                                              webUrl  \n",
            "0  https://www.tripadvisor.com/VacationRentalRevi...  \n",
            "1  https://www.tripadvisor.com/VacationRentalRevi...  \n",
            "2  https://www.tripadvisor.com/VacationRentalRevi...  \n",
            "3  https://www.tripadvisor.com/VacationRentalRevi...  \n",
            "4  https://www.tripadvisor.com/VacationRentalRevi...  \n",
            "\n",
            "[5 rows x 27 columns]\n",
            "Preprocessing complete. Cleaned data saved to '/content/preprocessed_vacation_rental.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/colomboRestaurants.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows to understand the structure of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Renaming Columns for Consistency: Replace '/' with '_'\n",
        "# This step standardizes column names by replacing '/' with '_' to avoid issues in processing\n",
        "df.columns = [col.strip().lower().replace(' ', '_').replace('/', '_') for col in df.columns]\n",
        "\n",
        "# Append 'addressobj_postalcode' to 'address', separated by a comma, if the column exists\n",
        "# This step checks if 'addressobj_postalcode' is in the dataframe\n",
        "if 'addressobj_postalcode' in df.columns:\n",
        "    # Combine 'address' and 'addressobj_postalcode', separating them with a comma if both are non-empty\n",
        "    df['address'] = df.apply(\n",
        "        lambda row: f\"{row['address']}, {row['addressobj_postalcode']}\" if pd.notna(row['addressobj_postalcode']) else row['address'],\n",
        "        axis=1\n",
        "    )\n",
        "    # Drop the 'addressobj_postalcode' column after appending its data\n",
        "    df = df.drop(columns=['addressobj_postalcode'])\n",
        "\n",
        "# Combine 'addressobj_street2' into 'addressobj_street1' and rename it to 'addressobj_street'\n",
        "# This step checks if 'addressobj_street2' is in the dataframe\n",
        "if 'addressobj_street2' in df.columns and 'addressobj_street1' in df.columns:\n",
        "    # Combine 'addressobj_street1' and 'addressobj_street2', separated by a comma if both are non-empty\n",
        "    df['addressobj_street1'] = df.apply(\n",
        "        lambda row: f\"{row['addressobj_street1']}, {row['addressobj_street2']}\" if pd.notna(row['addressobj_street2']) else row['addressobj_street1'],\n",
        "        axis=1\n",
        "    )\n",
        "    # Rename 'addressobj_street1' to 'addressobj_street'\n",
        "    df.rename(columns={'addressobj_street1': 'addressobj_street'}, inplace=True)\n",
        "    # Drop the 'addressobj_street2' column after appending its data\n",
        "    df = df.drop(columns=['addressobj_street2'])\n",
        "\n",
        "# Drop unnecessary columns related to ancestor locations\n",
        "# These columns are no longer needed in the dataset\n",
        "columns_to_drop = [\n",
        "    'ancestorlocations_0_id', 'ancestorlocations_0_name', 'ancestorlocations_0_subcategory',\n",
        "    'ancestorlocations_1_id', 'ancestorlocations_1_name', 'ancestorlocations_1_subcategory',\n",
        "    'ancestorlocations_2_id', 'ancestorlocations_2_name', 'ancestorlocations_2_subcategory',\n",
        "    'ancestorlocations_3_id', 'ancestorlocations_3_name', 'ancestorlocations_3_subcategory',\n",
        "\n",
        "]\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "\n",
        "# Combine cuisines columns into a single column\n",
        "# Identify columns starting with 'cuisines_' (after renaming)\n",
        "cuisines_columns = [col for col in df.columns if col.startswith('cuisines_')]\n",
        "\n",
        "# Create a new column 'cuisines_' by concatenating all the 'cuisines_' columns, separated by commas\n",
        "# Non-NaN values in the 'cuisines_' columns are joined for each row\n",
        "df['cuisines'] = df[cuisines_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original amenities columns\n",
        "# Remove the now redundant 'amenities_' columns to keep the dataset clean\n",
        "df = df.drop(columns=cuisines_columns)\n",
        "\n",
        "\n",
        "# Combine dietaryrestrictions columns into a single column\n",
        "# Identify columns starting with 'dietaryrestrictions_' (after renaming)\n",
        "dietaryrestrictions_columns = [col for col in df.columns if col.startswith('dietaryrestrictions_')]\n",
        "\n",
        "# Create a new column 'dietaryrestrictions_' by concatenating all the 'dietaryrestrictions_' columns, separated by commas\n",
        "# Non-NaN values in the 'dietaryrestrictions_' columns are joined for each row\n",
        "df['dietaryrestrictions'] = df[dietaryrestrictions_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original amenities columns\n",
        "# Remove the now redundant 'amenities_' columns to keep the dataset clean\n",
        "df = df.drop(columns=dietaryrestrictions_columns)\n",
        "\n",
        "\n",
        "# Combine dishes columns into a single column\n",
        "# Identify columns starting with 'dishes_' (after renaming)\n",
        "dishes_columns = [col for col in df.columns if col.startswith('dishes_')]\n",
        "\n",
        "# Create a new column 'dishes' by concatenating all the 'dishes_' columns, separated by commas\n",
        "# Non-NaN values in the 'dishes_' columns are joined for each row\n",
        "df['dishes'] = df[dishes_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original amenities columns\n",
        "# Remove the now redundant 'amenities_' columns to keep the dataset clean\n",
        "df = df.drop(columns=dishes_columns)\n",
        "\n",
        "\n",
        "# Combine dishes columns into a single column\n",
        "# Identify columns starting with 'features_' (after renaming)\n",
        "features_columns = [col for col in df.columns if col.startswith('features_')]\n",
        "\n",
        "# Create a new column 'features' by concatenating all the 'features_' columns, separated by commas\n",
        "# Non-NaN values in the 'features_' columns are joined for each row\n",
        "df['features'] = df[features_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original features columns\n",
        "# Remove the now redundant 'features_' columns to keep the dataset clean\n",
        "df = df.drop(columns=features_columns)\n",
        "\n",
        "\n",
        "mealtypes_columns = [col for col in df.columns if col.startswith('mealtypes_')]\n",
        "\n",
        "df['mealtypes'] = df[mealtypes_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "df = df.drop(columns=mealtypes_columns)\n",
        "\n",
        "\n",
        "establishmenttypes_columns = [col for col in df.columns if col.startswith('establishmenttypes_')]\n",
        "\n",
        "df['establishmenttypes'] = df[establishmenttypes_columns].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "df = df.drop(columns=establishmenttypes_columns)\n",
        "\n",
        "# Define the columns to extract and keep\n",
        "openhours_columns = [col for col in df.columns if \"openhours\" in col]\n",
        "closehours_columns = [col for col in df.columns if \"closehours\" in col]\n",
        "\n",
        "# Ensure there are columns to extract\n",
        "if openhours_columns and closehours_columns:\n",
        "    # Extract one value from openhours and closehours fields\n",
        "    # Take the first available non-NaN value from openhours and closehours\n",
        "    df['open_hour'] = df[openhours_columns].bfill(axis=1).iloc[:, 0]\n",
        "    df['close_hour'] = df[closehours_columns].bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "    # Drop all original openhours and closehours fields\n",
        "    columns_to_drop = openhours_columns + closehours_columns\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Print final columns to verify\n",
        "print(df.columns)\n",
        "\n",
        "# Update certain columns if they are empty\n",
        "# Combine 'weburl' and 'website' values if present and append to a default message\n",
        "if 'description' in df.columns:\n",
        "    df['description'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['description']) else row['description']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'email' in df.columns:\n",
        "    df['email'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['email']) else row['email']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'dietaryrestrictions' in df.columns:\n",
        "    df['dietaryrestrictions'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['dietaryrestrictions']) else row['dietaryrestrictions']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'dishes' in df.columns:\n",
        "    df['dishes'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['dishes']) else row['dishes']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'menuweburl' in df.columns:\n",
        "    df['menuweburl'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['menuweburl']) else row['menuweburl']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'phone' in df.columns:\n",
        "    df['phone'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['phone']) else row['phone']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'pricelevel' in df.columns:\n",
        "    df['pricelevel'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['pricelevel']) else row['pricelevel']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'subcategories_0' in df.columns:\n",
        "    df['subcategories_0'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['subcategories_0']) else row['subcategories_0']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'website' in df.columns:\n",
        "    df['website'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['website']) else row['website']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'open_hour' in df.columns:\n",
        "    df['open_hour'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['open_hour']) else row['open_hour']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'close_hour' in df.columns:\n",
        "    df['close_hour'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['close_hour']) else row['close_hour']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'dietaryrestrictions' in df.columns:\n",
        "    df['dietaryrestrictions'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['dietaryrestrictions']) else row['dietaryrestrictions']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'dishes' in df.columns:\n",
        "    df['dishes'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['dishes']) else row['dishes']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "\n",
        "if 'features' in df.columns:\n",
        "    df['features'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['features']) else row['features']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "if 'mealtypes' in df.columns:\n",
        "    df['mealtypes'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['mealtypes']) else row['mealtypes']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# These columns are no longer needed in the dataset\n",
        "columns_to_drop = ['hours', 'input', 'isclaimedicon', 'isclaimedtext', 'isclosed', 'islongclosed',\n",
        "                   'isnearbyresult', 'localaddress', 'localname', 'opennowtext', 'orderonline_0_buttontext',\n",
        "                   'orderonline_0_canprovidetimeslots', 'orderonline_0_headertext', 'orderonline_0_logourl',\n",
        "                   'orderonline_0_offerurl',\t'orderonline_0_provider',\t'orderonline_0_providerdisplayname',\n",
        "                   'orderonline_0_providerid',\t'orderonline_0_providertype',\t'ownerstopreasons'\t'ownerstopreasons_sectionheader',\n",
        "                   'ownerstopreasons_sponsoredby',\t'ownerstopreasons_topreasons_0_header',\t'ownerstopreasons_topreasons_0_image_url',\n",
        "                   'ownerstopreasons_topreasons_0_issearchterm',\t'ownerstopreasons_topreasons_0_keyword',\t'ownerstopreasons_topreasons_0_linktext',\n",
        "                   'ownerstopreasons_topreasons_0_rank',\t'ownerstopreasons_topreasons_0_review_rating',\t'ownerstopreasons_topreasons_0_review_reviewid',\n",
        "                   'ownerstopreasons_topreasons_0_review_screenname', 'ownerstopreasons_topreasons_0_text',\n",
        "                   'ownerstopreasons_topreasons_1_header',\t'ownerstopreasons_topreasons_1_image_url',\t'ownerstopreasons_topreasons_1_issearchterm',\n",
        "                   'ownerstopreasons_topreasons_1_keyword\townerstopreasons_topreasons_1_linktext',\n",
        "                   'ownerstopreasons_topreasons_1_rank',\t'ownerstopreasons_topreasons_1_review_rating',\n",
        "                   'ownerstopreasons_topreasons_1_review_reviewid',\t'ownerstopreasons_topreasons_1_review_screenname',\t'ownerstopreasons_topreasons_1_text',\n",
        "                   'ownerstopreasons_topreasons_2_header',\t'ownerstopreasons_topreasons_2_image_url',\t'ownerstopreasons_topreasons_2_issearchterm',\n",
        "                   'ownerstopreasons_topreasons_2_keyword',\t'ownerstopreasons_topreasons_2_linktext',\t'ownerstopreasons_topreasons_2_rank',\t'ownerstopreasons_topreasons_2_review_rating',\n",
        "                   'ownerstopreasons_topreasons_2_review_reviewid',\t'ownerstopreasons_topreasons_2_review_screenname',\t'ownerstopreasons_topreasons_2_text',\n",
        "                   'ownerstopreasons',\t'ownerstopreasons_sectionheader',\t'ownerstopreasons_topreasons_1_keyword',\t'ownerstopreasons_topreasons_1_linktext', 'photocount',\n",
        "                   'pricerange', 'hours_timezone', 'hours_weekranges_0_0_close', 'hours_weekranges_0_0_open', 'hours_weekranges_0_1_close', 'hours_weekranges_0_1_open', 'hours_weekranges_0_2_close', 'hours_weekranges_0_2_open',\n",
        "                   'hours_weekranges_1_0_close', 'hours_weekranges_1_0_open', 'hours_weekranges_1_1_close', 'hours_weekranges_1_1_open', 'hours_weekranges_1_2_close', 'hours_weekranges_1_2_open',\n",
        "                   'hours_weekranges_2_0_close', 'hours_weekranges_2_0_open', 'hours_weekranges_2_1_close', 'hours_weekranges_2_1_open', 'hours_weekranges_2_2_close', 'hours_weekranges_2_2_open',\n",
        "                    'hours_weekranges_3_0_close', 'hours_weekranges_3_0_open', 'hours_weekranges_3_1_close', 'hours_weekranges_3_1_open', 'hours_weekranges_3_2_close', 'hours_weekranges_3_2_open',\n",
        "                   'hours_weekranges_4_0_close', 'hours_weekranges_4_0_open', 'hours_weekranges_4_1_close', 'hours_weekranges_4_1_open', 'hours_weekranges_4_2_close', 'hours_weekranges_4_2_open',\n",
        "                   'hours_weekranges_5_0_close', 'hours_weekranges_5_0_open', 'hours_weekranges_5_1_close', 'hours_weekranges_5_1_open', 'hours_weekranges_5_2_close', 'hours_weekranges_5_2_open',\n",
        "                   'hours_weekranges_6_0_close', 'hours_weekranges_6_0_open', 'hours_weekranges_6_1_close', 'hours_weekranges_6_1_open', 'hours_weekranges_6_2_close', 'hours_weekranges_6_2_open'\n",
        "]\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "\n",
        "# Combine review and text columns for each review tag into a single column\n",
        "# Iterate through pairs of 'reviewtags_*_reviews' and 'reviewtags_*_text'\n",
        "review_columns = [col for col in df.columns if col.startswith('reviewtags_') and ('_reviews' in col or '_text' in col)]\n",
        "review_pairs = {}\n",
        "\n",
        "# Group review and text columns into pairs (e.g., 'reviewtags_0_reviews', 'reviewtags_0_text')\n",
        "for col in review_columns:\n",
        "    key = col.split('_')[1]  # Extract the common index for pairing\n",
        "    review_pairs.setdefault(key, []).append(col)\n",
        "\n",
        "# Create a new column 'reviews_and_text' to store combined reviews and text\n",
        "df['reviews_and_text'] = df.apply(\n",
        "    lambda row: ', '.join(\n",
        "        f\"{row[pair[0]]}: {row[pair[1]]}\" for pair in review_pairs.values()\n",
        "        if len(pair) == 2 and pd.notna(row[pair[0]]) and pd.notna(row[pair[1]])\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Drop original review and text columns to clean the dataset\n",
        "df = df.drop(columns=review_columns)\n",
        "\n",
        "columns_to_replace = [\n",
        "    'latitude', 'longitude', 'travelerchoiceaward'\n",
        "]\n",
        "\n",
        "# Replace missing values: text columns with 'not provided', numeric score columns with 0\n",
        "for col in columns_to_replace:\n",
        "    if col in df.columns:\n",
        "      df[col] = df[col].fillna('not provided')\n",
        "\n",
        "if 'reviews_and_text' in df.columns:\n",
        "    df['reviews_and_text'] = df.apply(\n",
        "        lambda row: (\n",
        "            f\"Please visit the following links for more details: {', '.join(filter(pd.notna, [row.get('weburl', ''), row.get('website', '')]))}\"\n",
        "            if pd.isna(row['reviews_and_text']) else row['reviews_and_text']\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "\n",
        "# Save the modified data to a new Excel file\n",
        "# The preprocessed file contains standardized column names, a combined 'amenities' column,\n",
        "# an updated 'address' column, and an updated 'addressobj_street' column\n",
        "output_file = '/content/preprocessed_colombo_restaurant.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Preprocessing complete. Cleaned data saved to '{output_file}'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny4rEDJIgLuO",
        "outputId": "caa652b5-bdf0-4268-a7a7-23e28f241869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "                                             address addressObj/city  \\\n",
            "0  57 Ward Place Roof Top, Jetwing Colombo Seven,...         Colombo   \n",
            "1              2 Galle Road, Colombo 00300 Sri Lanka         Colombo   \n",
            "2  106 Thimbirigasyaya Road, Colombo 00500 Sri Lanka         Colombo   \n",
            "3       10 Galle Face Drive, Colombo 00300 Sri Lanka         Colombo   \n",
            "4  590 Colombo - Galle Main Road Marino Mall, Col...         Colombo   \n",
            "\n",
            "  addressObj/country  addressObj/postalcode             addressObj/street1  \\\n",
            "0          Sri Lanka                  700.0                  57 Ward Place   \n",
            "1          Sri Lanka                  300.0                   2 Galle Road   \n",
            "2          Sri Lanka                  500.0       106 Thimbirigasyaya Road   \n",
            "3          Sri Lanka                  300.0            10 Galle Face Drive   \n",
            "4          Sri Lanka                    NaN  590 Colombo - Galle Main Road   \n",
            "\n",
            "                addressObj/street2  ancestorLocations/0/id  \\\n",
            "0  Roof Top, Jetwing Colombo Seven                  293962   \n",
            "1                              NaN                  293962   \n",
            "2                              NaN                  293962   \n",
            "3                              NaN                  293962   \n",
            "4                      Marino Mall                  293962   \n",
            "\n",
            "  ancestorLocations/0/name ancestorLocations/0/subcategory  \\\n",
            "0                  Colombo                            City   \n",
            "1                  Colombo                            City   \n",
            "2                  Colombo                            City   \n",
            "3                  Colombo                            City   \n",
            "4                  Colombo                            City   \n",
            "\n",
            "   ancestorLocations/1/id  ... reviewTags/27/text reviewTags/28/reviews  \\\n",
            "0                 2467812  ...                NaN                   NaN   \n",
            "1                 2467812  ...                NaN                   NaN   \n",
            "2                 2467812  ...                NaN                   NaN   \n",
            "3                 2467812  ...                NaN                   NaN   \n",
            "4                 2467812  ...                NaN                   NaN   \n",
            "\n",
            "   reviewTags/28/text reviewTags/29/reviews reviewTags/29/text  \\\n",
            "0                 NaN                   NaN                NaN   \n",
            "1                 NaN                   NaN                NaN   \n",
            "2                 NaN                   NaN                NaN   \n",
            "3                 NaN                   NaN                NaN   \n",
            "4                 NaN                   NaN                NaN   \n",
            "\n",
            "   subcategories/0 travelerChoiceAward        type  \\\n",
            "0              NaN                 NaN  RESTAURANT   \n",
            "1              NaN                 NaN  RESTAURANT   \n",
            "2              NaN                 NaN  RESTAURANT   \n",
            "3         Sit down                 NaN  RESTAURANT   \n",
            "4              NaN                 NaN  RESTAURANT   \n",
            "\n",
            "                                              webUrl  \\\n",
            "0  https://www.tripadvisor.com/Restaurant_Review-...   \n",
            "1  https://www.tripadvisor.com/Restaurant_Review-...   \n",
            "2  https://www.tripadvisor.com/Restaurant_Review-...   \n",
            "3  https://www.tripadvisor.com/Restaurant_Review-...   \n",
            "4  https://www.tripadvisor.com/Restaurant_Review-...   \n",
            "\n",
            "                                             website  \n",
            "0  https://www.jetwinghotels.com/jetwingcolombose...  \n",
            "1           https://www.facebook.com/1864restaurant/  \n",
            "2          http://www.facebook.com/bubblemebubbletea  \n",
            "3                                http://barracuda.lk  \n",
            "4                                                NaN  \n",
            "\n",
            "[5 rows x 319 columns]\n",
            "Index(['address', 'addressobj_city', 'addressobj_country', 'addressobj_street',\n",
            "       'category', 'description', 'email', 'hours', 'hours_timezone',\n",
            "       'hours_weekranges_0_0_close',\n",
            "       ...\n",
            "       'weburl', 'website', 'cuisines', 'dietaryrestrictions', 'dishes',\n",
            "       'features', 'mealtypes', 'establishmenttypes', 'open_hour',\n",
            "       'close_hour'],\n",
            "      dtype='object', length=196)\n",
            "Preprocessing complete. Cleaned data saved to '/content/preprocessed_colombo_restaurant.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/user_inputs.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows to understand the structure of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Renaming Columns for Consistency: Replace '/' with '_'\n",
        "# This step standardizes column names by replacing '/' with '_' to avoid issues in processing\n",
        "df.columns = [col.strip().lower().replace(' ', '_').replace('/', '_') for col in df.columns]\n",
        "\n",
        "# These columns are no longer needed in the dataset\n",
        "columns_to_drop = ['timestamp']\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "\n",
        "columns_to_replace = [\n",
        "    'is_there_anything_else_you_would_like_us_to_know_about_your_travel_preferences?'\n",
        "]\n",
        "\n",
        "# Replace missing values: text columns with 'not provided', numeric score columns with 0\n",
        "for col in columns_to_replace:\n",
        "    if col in df.columns:\n",
        "      df[col] = df[col].fillna('not provided')\n",
        "\n",
        "# Save the modified data to a new Excel file\n",
        "# The preprocessed file contains standardized column names, a combined 'amenities' column,\n",
        "# an updated 'address' column, and an updated 'addressobj_street' column\n",
        "output_file = '/content/preprocessed_user_inputs.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Preprocessing complete. Cleaned data saved to '{output_file}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m9cqAnaS3Mg",
        "outputId": "2596588c-ed3b-4879-bdde-d3d3f08f928b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "                Timestamp      Name   \\\n",
            "0 2024-11-04 10:43:35.738  Mathusha    \n",
            "1 2024-11-04 11:13:51.504     nielia   \n",
            "2 2024-11-04 11:14:01.859     Ehansa   \n",
            "3 2024-11-04 11:32:33.207     Oshini   \n",
            "4 2024-11-04 11:33:55.682       Umar   \n",
            "\n",
            "  Do you wish to have a trip itinerary generator website for your future trips?   \\\n",
            "0                                                Yes                               \n",
            "1                                                Yes                               \n",
            "2                                                Yes                               \n",
            "3                                                Yes                               \n",
            "4                                                Yes                               \n",
            "\n",
            "  How often do you travel per year?  How many people are traveling?   \\\n",
            "0               Rarely (1 - 2 times)                          Couple   \n",
            "1         Occasionally (3 - 5 times)                            Solo   \n",
            "2         Occasionally (3 - 5 times)          Family (3 - 5 members)   \n",
            "3         Occasionally (3 - 5 times)          Family (3 - 5 members)   \n",
            "4               Rarely (1 - 2 times)                     Group ( 6+)   \n",
            "\n",
            "  Which destination/s are you most interested in visiting?   \\\n",
            "0                                               Ella          \n",
            "1                                       Nuwara Eliya          \n",
            "2                                               Ella          \n",
            "3                                               Ella          \n",
            "4                                           Dambulla          \n",
            "\n",
            "   How many days you want to travel?  What is your age category?   \\\n",
            "0                                   2                     18 - 24   \n",
            "1                                   4                     18 - 24   \n",
            "2                                   3                    below 18   \n",
            "3                                   3                     18 - 24   \n",
            "4                                   3                     18 - 24   \n",
            "\n",
            "  What is your preferred accommodation type?   \\\n",
            "0                              Hotel / Resort   \n",
            "1                                       Villa   \n",
            "2                                 Guest House   \n",
            "3                                       Villa   \n",
            "4                                       Villa   \n",
            "\n",
            "  What is your estimated budget for accommodation per day?   \\\n",
            "0                                Rs. 1000 - Rs. 5000          \n",
            "1                                Rs. 1000 - Rs. 5000          \n",
            "2                               Rs. 5100 - Rs. 10000          \n",
            "3                               Rs. 5100 - Rs. 10000          \n",
            "4                                Rs. 1000 - Rs. 5000          \n",
            "\n",
            "  Which food category you prefer?  Which cuisine/s are you interested?   \\\n",
            "0                         Non-Veg           Sri Lankan, Indian, Italian   \n",
            "1                             Veg                            Sri Lankan   \n",
            "2                         Non-Veg                      Italian, Western   \n",
            "3                         Non-Veg  Sri Lankan, Indian, Chinese, Western   \n",
            "4                         Non-Veg             Chinese, Italian, Western   \n",
            "\n",
            "             Which activities are you interested in?  \\\n",
            "0  Nature Trails, Cultural Experiences, Adventuro...   \n",
            "1  Nature Trails, Cultural Experiences, Adventuro...   \n",
            "2            Adventurous, Shopping, Spa and Wellness   \n",
            "3  Historical Sites, Nature Trails, Shopping, Wil...   \n",
            "4    Nature Trails, Adventurous, Wildlife, Religious   \n",
            "\n",
            "  Do you prefer morning, afternoon, evening or night time for activities?   \\\n",
            "0                          Afternoon, Evening, Night                         \n",
            "1                                            Morning                         \n",
            "2                                     Evening, Night                         \n",
            "3                                          Afternoon                         \n",
            "4                                     Evening, Night                         \n",
            "\n",
            "  Which transportation modes are you preferred for traveling within the destination?    \\\n",
            "0      Public Bus, Public Train, Tuk-Tuk, Motor Bike                                     \n",
            "1                              Public Train, On foot                                     \n",
            "2                                        Private Car                                     \n",
            "3                                        Private Car                                     \n",
            "4                                       Public Train                                     \n",
            "\n",
            "  Are you travelling with children or pets?   \\\n",
            "0                                    Neither   \n",
            "1                                    Neither   \n",
            "2                                    Neither   \n",
            "3                                    Neither   \n",
            "4                                    Neither   \n",
            "\n",
            "  Is there anything else you would like us to know about your travel preferences?   \n",
            "0                                                NaN                                \n",
            "1                                                NaN                                \n",
            "2                                                NaN                                \n",
            "3                                                NaN                                \n",
            "4                                                NaN                                \n",
            "Preprocessing complete. Cleaned data saved to '/content/preprocessed_user_inputs.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-ranking\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "#import tensorflow_ranking as tfr\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "def load_excel(file_path):\n",
        "    return pd.read_excel(file_path)\n",
        "\n",
        "# Load datasets into pandas DataFrames\n",
        "hotels_df = load_excel('/content/preprocessed_hotel_data.xlsx')\n",
        "restaurants_df = load_excel('/content/preprocessed_colombo_restaurant.xlsx')\n",
        "users_df = load_excel('/content/preprocessed_user_inputs.xlsx')\n",
        "vacation_rentals_df = load_excel('/content/preprocessed_vacation_rental.xlsx')\n",
        "\n",
        "# Extract necessary fields\n",
        "hotels_df = hotels_df[['id', 'name', 'type', 'rating']]\n",
        "restaurants_df = restaurants_df[['id', 'name', 'type', 'rating']]\n",
        "vacation_rentals_df = vacation_rentals_df[['id', 'name', 'type', 'rating']]\n",
        "users_df = users_df[['user_id', 'preferred_destination_id', 'rating']]\n",
        "\n",
        "# Combine destination datasets\n",
        "destinations_df = pd.concat([hotels_df, restaurants_df, vacation_rentals_df])\n",
        "\n",
        "# Prepare TensorFlow Datasets\n",
        "ratings = tf.data.Dataset.from_tensor_slices({\n",
        "    \"user_id\": users_df['user_id'].astype(str).to_numpy(),\n",
        "    \"destination_id\": users_df['preferred_destination_id'].astype(str).to_numpy(),\n",
        "    \"user_rating\": users_df['rating'].to_numpy()\n",
        "})\n",
        "\n",
        "destinations = tf.data.Dataset.from_tensor_slices(destinations_df['name'].astype(str).to_numpy())\n",
        "users = ratings.map(lambda x: x[\"user_id\"])\n",
        "\n",
        "# Vocabulary setup for users and destinations\n",
        "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "user_ids_vocabulary.adapt(users.batch(1000))\n",
        "\n",
        "destination_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "destination_ids_vocabulary.adapt(destinations.batch(1000))\n",
        "\n",
        "# Group dataset for training\n",
        "key_func = lambda x: user_ids_vocabulary(x[\"user_id\"])\n",
        "reduce_func = lambda key, dataset: dataset.batch(100)\n",
        "ds_train = ratings.group_by_window(\n",
        "    key_func=key_func, reduce_func=reduce_func, window_size=100)\n",
        "\n",
        "# Feature-label split function\n",
        "def _features_and_labels(x):\n",
        "    labels = x.pop(\"user_rating\")\n",
        "    return x, labels\n",
        "\n",
        "ds_train = ds_train.map(_features_and_labels)\n",
        "ds_train = ds_train.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=32))\n",
        "\n",
        "# Define Travel Itinerary Recommendation Model\n",
        "class ItineraryRankingModel(tf.keras.Model):\n",
        "    def __init__(self, user_vocab, destination_vocab):\n",
        "        super().__init__()\n",
        "        self.user_vocab = user_vocab\n",
        "        self.destination_vocab = destination_vocab\n",
        "        self.user_embed = tf.keras.layers.Embedding(user_vocab.vocabulary_size(), 64)\n",
        "        self.destination_embed = tf.keras.layers.Embedding(destination_vocab.vocabulary_size(), 64)\n",
        "\n",
        "    def call(self, features):\n",
        "        user_embeddings = self.user_embed(self.user_vocab(features[\"user_id\"]))\n",
        "        destination_embeddings = self.destination_embed(self.destination_vocab(features[\"destination_id\"]))\n",
        "        return tf.reduce_sum(user_embeddings * destination_embeddings, axis=2)\n",
        "\n",
        "# Instantiate and compile the model\n",
        "model = ItineraryRankingModel(user_ids_vocabulary, destination_ids_vocabulary)\n",
        "optimizer = tf.keras.optimizers.Adagrad(0.5)\n",
        "loss = tfr.keras.losses.get(\n",
        "    loss=tfr.keras.losses.RankingLossKey.SOFTMAX_LOSS, ragged=True\n",
        ")\n",
        "eval_metrics = [\n",
        "    tfr.keras.metrics.get(key=\"ndcg\", name=\"metric/ndcg\", ragged=True),\n",
        "    tfr.keras.metrics.get(key=\"mrr\", name=\"metric/mrr\", ragged=True),\n",
        "]\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=eval_metrics)\n",
        "\n",
        "# Train the model\n",
        "model.fit(ds_train, epochs=3)\n",
        "\n",
        "# Prepare candidate destinations\n",
        "for destination_names in destinations.batch(2000):\n",
        "    break\n",
        "\n",
        "# Generate personalized recommendations for a specific user\n",
        "user_id = \"42\"  # Replace with desired user ID\n",
        "inputs = {\n",
        "    \"user_id\": tf.expand_dims(tf.repeat(user_id, repeats=destination_names.shape[0]), axis=0),\n",
        "    \"destination_id\": tf.expand_dims(destination_names, axis=0)\n",
        "}\n",
        "\n",
        "# Get recommendations\n",
        "scores = model(inputs)\n",
        "recommended_destinations = tfr.utils.sort_by_scores(scores, [tf.expand_dims(destination_names, axis=0)])[0]\n",
        "\n",
        "# Print the top 5 recommendations\n",
        "print(f\"Top 5 recommendations for user {user_id}: {recommended_destinations[0, :5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "yDNzmRu0aleL",
        "outputId": "0105fa9a-3f92-46e6-e7e5-800503b82dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['actual_user_id_column', 'actual_preferred_destination_column',\\n       'actual_rating_column'],\\n      dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e77c82482191>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mrestaurants_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurants_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mvacation_rentals_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvacation_rentals_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0musers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musers_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_user_id_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actual_preferred_destination_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actual_rating_column'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['actual_user_id_column', 'actual_preferred_destination_column',\\n       'actual_rating_column'],\\n      dtype='object')] are in the [columns]\""
          ]
        }
      ]
    }
  ]
}