{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy77dHfAGkHyUjU0s41Ujt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathu3004/Pearl_Path/blob/Chatbot/ChatbotModelTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SVYBsR9pxlC",
        "outputId": "9621c212-c58b-4604-89df-65bd59902008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Download and load the SQuAD dataset\n",
        "def load_squad_dataset():\n",
        "    url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    return data\n",
        "\n",
        "# Preprocess the SQuAD dataset\n",
        "def preprocess_squad_data(data):\n",
        "    questions = []\n",
        "    contexts = []\n",
        "    answers = []\n",
        "\n",
        "    for article in data['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                if qa['answers']:  # Check if there are any answers\n",
        "                    answer = qa['answers'][0]['text']\n",
        "                    questions.append(question)\n",
        "                    contexts.append(context)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return questions, contexts, answers\n",
        "\n",
        "# Initialize DPR models and tokenizers\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "\n",
        "# Initialize T5 model and tokenizer\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "# Load and preprocess the SQuAD dataset\n",
        "squad_data = load_squad_dataset()\n",
        "questions, contexts, answers = preprocess_squad_data(squad_data)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_questions, test_questions, train_contexts, test_contexts, train_answers, test_answers = train_test_split(\n",
        "    questions, contexts, answers, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Encode questions and contexts for training data\n",
        "train_question_embeddings = []\n",
        "train_context_embeddings = []\n",
        "\n",
        "for question in train_questions:\n",
        "    inputs = question_tokenizer(question, return_tensors='pt')\n",
        "    outputs = question_encoder(**inputs)\n",
        "    train_question_embeddings.append(outputs.pooler_output.detach().numpy())\n",
        "\n",
        "for context in train_contexts:\n",
        "    inputs = context_tokenizer(context, return_tensors='pt')\n",
        "    outputs = context_encoder(**inputs)\n",
        "    train_context_embeddings.append(outputs.pooler_output.detach().numpy())\n",
        "\n",
        "train_question_embeddings = np.array(train_question_embeddings).squeeze()\n",
        "train_context_embeddings = np.array(train_context_embeddings).squeeze()\n",
        "\n",
        "# Compute similarity scores for training data\n",
        "train_similarity_scores = np.dot(train_question_embeddings, train_context_embeddings.T)\n",
        "\n",
        "# Retrieve top-k contexts for each question in training data\n",
        "top_k = 1\n",
        "train_retrieved_contexts = []\n",
        "for i in range(len(train_questions)):\n",
        "    top_context_indices = train_similarity_scores[i].argsort()[-top_k:][::-1]\n",
        "    train_retrieved_contexts.append([train_contexts[idx] for idx in top_context_indices])\n",
        "\n",
        "# Generate answers using T5 model for training data\n",
        "train_generated_answers = []\n",
        "for i, context in enumerate(train_retrieved_contexts):\n",
        "    input_text = f\"question: {train_questions[i]} context: {' '.join(context)}\"\n",
        "    inputs = t5_tokenizer(input_text, return_tensors='pt')\n",
        "    outputs = t5_model.generate(**inputs)\n",
        "    generated_answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    train_generated_answers.append(generated_answer)\n",
        "\n",
        "# Evaluate accuracy on training data\n",
        "train_exact_match = accuracy_score(train_answers, train_generated_answers)\n",
        "train_precision = precision_score(train_answers, train_generated_answers, average='weighted')\n",
        "train_recall = recall_score(train_answers, train_generated_answers, average='weighted')\n",
        "train_f1 = f1_score(train_answers, train_generated_answers, average='weighted')\n",
        "train_conf_matrix = confusion_matrix(train_answers, train_generated_answers, labels=np.unique(train_answers))\n",
        "\n",
        "print(f\"Training Exact Match: {train_exact_match}\")\n",
        "print(f\"Training Precision: {train_precision}\")\n",
        "print(f\"Training Recall: {train_recall}\")\n",
        "print(f\"Training F1 Score: {train_f1}\")\n",
        "print(f\"Training Confusion Matrix:\\n{train_conf_matrix}\")\n",
        "\n",
        "# Encode questions and contexts for testing data\n",
        "test_question_embeddings = []\n",
        "test_context_embeddings = []\n",
        "\n",
        "for question in test_questions:\n",
        "    inputs = question_tokenizer(question, return_tensors='pt')\n",
        "    outputs = question_encoder(**inputs)\n",
        "    test_question_embeddings.append(outputs.pooler_output.detach().numpy())\n",
        "\n",
        "for context in test_contexts:\n",
        "    inputs = context_tokenizer(context, return_tensors='pt')\n",
        "    outputs = context_encoder(**inputs)\n",
        "    test_context_embeddings.append(outputs.pooler_output.detach().numpy())\n",
        "\n",
        "test_question_embeddings = np.array(test_question_embeddings).squeeze()\n",
        "test_context_embeddings = np.array(test_context_embeddings).squeeze()\n",
        "\n",
        "# Compute similarity scores for testing data\n",
        "test_similarity_scores = np.dot(test_question_embeddings, test_context_embeddings.T)\n",
        "\n",
        "# Retrieve top-k contexts for each question in testing data\n",
        "test_retrieved_contexts = []\n",
        "for i in range(len(test_questions)):\n",
        "    top_context_indices = test_similarity_scores[i].argsort()[-top_k:][::-1]\n",
        "    test_retrieved_contexts.append([test_contexts[idx] for idx in top_context_indices])\n",
        "\n",
        "# Generate answers using T5 model for testing data\n",
        "test_generated_answers = []\n",
        "for i, context in enumerate(test_retrieved_contexts):\n",
        "    input_text = f\"question: {test_questions[i]} context: {' '.join(context)}\"\n",
        "    inputs = t5_tokenizer(input_text, return_tensors='pt')\n",
        "    outputs = t5_model.generate(**inputs)\n",
        "    generated_answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    test_generated_answers.append(generated_answer)\n",
        "\n",
        "# Evaluate accuracy on testing data\n",
        "test_exact_match = accuracy_score(test_answers, test_generated_answers)\n",
        "test_precision = precision_score(test_answers, test_generated_answers, average='weighted')\n",
        "test_recall = recall_score(test_answers, test_generated_answers, average='weighted')\n",
        "test_f1 = f1_score(test_answers, test_generated_answers, average='weighted')\n",
        "test_conf_matrix = confusion_matrix(test_answers, test_generated_answers, labels=np.unique(test_answers))\n",
        "\n",
        "print(f\"Testing Exact Match: {test_exact_match}\")\n",
        "print(f\"Testing Precision: {test_precision}\")\n",
        "print(f\"Testing Recall: {test_recall}\")\n",
        "print(f\"Testing F1 Score: {test_f1}\")\n",
        "print(f\"Testing Confusion Matrix:\\n{test_conf_matrix}\")\n",
        "\n",
        "# Visualization of Accuracy Metrics for Testing Data\n",
        "metrics = ['Exact Match', 'Precision', 'Recall', 'F1 Score']\n",
        "test_scores = [test_exact_match, test_precision, test_recall, test_f1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, test_scores, color=['blue', 'green', 'red', 'purple'])\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Score')\n",
        "plt.title('Accuracy Metrics for DPR and T5 Models on Testing Data')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for i, score in enumerate(test_scores):\n",
        "    plt.text(i, score + 0.01, f'{score:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization of Confusion Matrix for Testing Data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(test_answers), yticklabels=np.unique(test_answers))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix for Testing Data')\n",
        "plt.show()\n"
      ]
    }
  ]
}